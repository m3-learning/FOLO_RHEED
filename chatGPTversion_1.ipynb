{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 18:15:17.402059: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 18:15:17.909737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The below code uses the \"traditional\" method to provide close fits that the model uses for standardization later.\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class HDF5Dataset(Sequence):\n",
    "    def __init__(self, file_path, batch_size=1000):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as h5:\n",
    "            data = []\n",
    "            growth_numbers = []\n",
    "\n",
    "            # Iterate through the groups and match the pattern\n",
    "            for key in h5.keys():\n",
    "                match = re.match(r'growth_(\\d+)', key)\n",
    "                if match:\n",
    "                    growth_number = int(match.group(1))\n",
    "                    growth_numbers.append(growth_number)\n",
    "\n",
    "                    # Extract the corresponding dataset\n",
    "                    dataset = np.array(h5[key][\"spot_2\"])\n",
    "                    data.append(dataset)\n",
    "                    \n",
    "            # Concatenate all the datasets and normalize\n",
    "            self.data = np.vstack(data)\n",
    "            self.data = self.data.astype(np.float32)\n",
    "            self.data_max = np.max(self.data)\n",
    "            self.data /= self.data_max\n",
    "       \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_data = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return batch_data, np.full(len(batch_data), self.data_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 234.98 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 229.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 202.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 232.81 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the path to the HDF5 file\n",
    "file_path = '/home/ferroelectric/sean/RHEED_4848_test6.h5'\n",
    "\n",
    "# Create an instance of the HDF5Dataset generator\n",
    "batch_size = 1000\n",
    "data_generator = HDF5Dataset(file_path, batch_size=batch_size)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Defining the 2D Gaussian function\n",
    "def gaussian2D(x_y, A, x0, y0, sigma_x, sigma_y):\n",
    "    x, y = x_y\n",
    "    return A * np.exp(-((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2)))\n",
    "\n",
    "# Computing residuals for least-squares minimization\n",
    "def residuals(params, x, y, data):\n",
    "    A, x0, y0, sigma_x, sigma_y = params\n",
    "    model = gaussian2D((x, y), A, x0, y0, sigma_x, sigma_y)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "def convert_parameters(parameters):\n",
    "    A, x0, y0, sigma_x, sigma_y = parameters\n",
    "    \n",
    "    mean_x = x0\n",
    "    mean_y = y0\n",
    "    cov_x = sigma_x\n",
    "    cov_y = sigma_y\n",
    "    \n",
    "    # Calculate theta from the covariance values\n",
    "    if cov_x != 0 and cov_y != 0:\n",
    "        theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2) + 1e-9)\n",
    "    else:\n",
    "        theta = 0.0\n",
    "    \n",
    "    return mean_x, mean_y, cov_x, cov_y, theta\n",
    "\n",
    "# Fitting multiple images using Dask\n",
    "def fit_and_convert_parameters(h5_filename):\n",
    "    all_converted_params = []  \n",
    "    all_images = []\n",
    "    \n",
    "    with h5py.File(h5_filename, 'r') as h5_file:\n",
    "        for group_num in range(1, 13):  \n",
    "            group_name = f'growth_{group_num}'\n",
    "            if group_name in h5_file:\n",
    "                group = h5_file[group_name]['spot_2']\n",
    "\n",
    "                images = group[:]  \n",
    "                \n",
    "                normalized_images = [image / image.max() for image in images]  \n",
    "                all_images.extend(normalized_images)\n",
    "                \n",
    "                guesses = [add_guess(image) for image in normalized_images]\n",
    "                fits = [fit_gaussian2D_delayed(image, guess) for image, guess in zip(normalized_images, guesses)]\n",
    "                \n",
    "                converted_params = [convert_parameters(params) for params in compute(*fits)]\n",
    "                all_converted_params.extend(converted_params)  \n",
    "\n",
    "    return all_converted_params, all_images\n",
    "\n",
    "# Parallelizing the fit function using Dask's delayed\n",
    "@delayed\n",
    "def fit_gaussian2D_delayed(data, guess):\n",
    "    y, x = np.indices(data.shape)\n",
    "    result = least_squares(residuals, guess, args=(x, y, data))\n",
    "    return result.x\n",
    "\n",
    "# Adding an initial guess for the fit\n",
    "def add_guess(data):\n",
    "    A_guess = np.max(data)\n",
    "    x0_guess, y0_guess = np.unravel_index(np.argmax(data), data.shape)\n",
    "    sigma_x_guess = sigma_y_guess = np.std(data)\n",
    "    return [A_guess, x0_guess, y0_guess, sigma_x_guess, sigma_y_guess]\n",
    "\n",
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    h5_filename = \"/home/ferroelectric/sean/RHEED_4848_test6.h5\"  # Replace with the actual path\n",
    "    \n",
    "    with Client() as client:  # Starts a local cluster or connects to an existing one\n",
    "        results, images2 = fit_and_convert_parameters(h5_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the standard scaler being fit on the traditional data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "output_scaler = StandardScaler()\n",
    "data = results  # structured like [[meanx1, meany1, covx1, covy1, theta1], [meanx2, meany2, covx2, covy2, theta2]]\n",
    "output_scaler.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import math\n",
    "\n",
    "class LeNet5(tf.keras.Model):\n",
    "    def __init__(self, func, num_classes, testing=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.func = func\n",
    "        self.testing = testing\n",
    "        \n",
    "        self.layer1 = tf.keras.Sequential([\n",
    "            Conv2D(6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(pool_size=4, strides=4)\n",
    "        ])\n",
    "        \n",
    "        self.layer2 = tf.keras.Sequential([\n",
    "            Conv2D(16, kernel_size=5, strides=1, padding='valid'),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(pool_size=2, strides=2)\n",
    "        ])\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc = Dense(98)\n",
    "        self.relu = ReLU()\n",
    "        self.fc1 = Dense(52)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        embedding = self.fc2(out)\n",
    "        \n",
    "        # Assume output_scaler is defined and accessible\n",
    "        unscaled_param = embedding * tf.sqrt(output_scaler.var_) + output_scaler.mean_\n",
    "        unscaled_param = tf.Variable(unscaled_param)\n",
    "        unscaled_param[:, 4].assign((math.pi / 4) * (tf.nn.tanh(unscaled_param[:, 4]) + 1))\n",
    "        \n",
    "        final = self.func(unscaled_param)\n",
    "        \n",
    "        if self.testing:\n",
    "            return embedding, unscaled_param, final \n",
    "        \n",
    "        return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class LeNet5Inference(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5Inference, self).__init__()\n",
    "        self.layer1 = models.Sequential([\n",
    "            layers.Conv2D(6, kernel_size=5, strides=1, padding='valid', input_shape=(32, 32, 1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling2D(pool_size=4, strides=4)\n",
    "        ])\n",
    "        self.layer2 = models.Sequential([\n",
    "            layers.Conv2D(16, kernel_size=5, strides=1, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "        ])\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(98)\n",
    "        self.relu = layers.ReLU()\n",
    "        self.fc1 = layers.Dense(52)\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "        \n",
    "        self.output_scaler_var = tf.Variable([1.0], trainable=False)  # Placeholder for the variance\n",
    "        self.output_scaler_mean = tf.Variable([0.0], trainable=False)  # Placeholder for the mean\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        embedding = self.fc2(out)\n",
    "        embedding = embedding * tf.sqrt(self.output_scaler_var) + self.output_scaler_mean  # Unscaling\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaussianGenerator(Layer):\n",
    "    def __init__(self, img_dim):\n",
    "        super(GaussianGenerator, self).__init__()\n",
    "        self.img_dim = img_dim\n",
    "\n",
    "    def call(self, params):\n",
    "        batch_size = tf.shape(params)[0]\n",
    "        mean_x, mean_y, cov_x, cov_y, theta = tf.unstack(params, axis=-1)\n",
    "        cov_x = tf.clip_by_value(cov_x, clip_value_min=1e-9, clip_value_max=tf.float32.max)\n",
    "        cov_y = tf.clip_by_value(cov_y, clip_value_min=1e-9, clip_value_max=tf.float32.max)\n",
    "\n",
    "        x = tf.range(self.img_dim[1], dtype=tf.float32)\n",
    "        y = tf.range(self.img_dim[0], dtype=tf.float32)\n",
    "        x = tf.reshape(x, (-1, 1))\n",
    "        x = tf.tile(x, (1, self.img_dim[0]))\n",
    "        y = tf.reshape(y, (1, -1))\n",
    "        y = tf.tile(y, (self.img_dim[1], 1))\n",
    "        x = tf.tile(tf.expand_dims(x, 0), [batch_size, 1, 1])\n",
    "        y = tf.tile(tf.expand_dims(y, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        rota_matrix = tf.stack([tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)], axis=-1)\n",
    "        rota_matrix = tf.reshape(rota_matrix, (batch_size, 2, 2))\n",
    "\n",
    "        xy = tf.stack([x - tf.reshape(mean_x, (-1, 1, 1)), y - tf.reshape(mean_y, (-1, 1, 1))], axis=-1)\n",
    "        xy = tf.einsum('bijk,bkl->bijl', xy, rota_matrix)\n",
    "\n",
    "        img = tf.exp(-0.5 * (tf.square(xy[:, :, :, 0]) / tf.square(tf.reshape(cov_x, (-1, 1, 1))) + tf.square(xy[:, :, :, 1]) / tf.square(tf.reshape(cov_y, (-1, 1, 1)))))\n",
    "\n",
    "        return tf.expand_dims(img, axis=1)\n",
    "\n",
    "# Example usage\n",
    "img_dim = (48, 48)  # Replace with the desired image dimensions\n",
    "model = GaussianGenerator(img_dim)\n",
    "sample_params = tf.constant([[19.2763, 24.8520, 11.2061, 6.8914, 0.7006]], dtype=tf.float32)\n",
    "\n",
    "# Call the model to generate the Gaussian image\n",
    "generated_img = model(sample_params)\n",
    "\n",
    "# Display the generated image shape\n",
    "print(\"Generated Image Shape:\", generated_img.shape)\n",
    "plt.imshow(generated_img.numpy().squeeze(0).squeeze(0), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "    \"\"\"\n",
    "    Compute the custom weighted MSE loss between two grayscale images I and J.\n",
    "\n",
    "    Parameters:\n",
    "    - I: tf.Tensor of shape [batch_size, 1, M, N], the input image\n",
    "    - J: tf.Tensor of shape [batch_size, 1, M, N], the target image\n",
    "    - n: int, the exponent to raise the input image for the weight\n",
    "\n",
    "    Returns:\n",
    "    - loss: tf.Tensor, the computed loss\n",
    "    \"\"\"\n",
    "    # Compute the weight\n",
    "    W = tf.pow(I, n)\n",
    "\n",
    "    # Compute the squared differences\n",
    "    squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "    # Compute the weighted squared differences\n",
    "    weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Test the function\n",
    "I = tf.random.normal((16, 1, 128, 128))  # Batch of 16 128x128 grayscale images\n",
    "J = tf.random.normal((16, 1, 128, 128))  # Batch of 16 128x128 grayscale images\n",
    "n = 2  # Exponent value\n",
    "\n",
    "loss = custom_weighted_mse_loss(I, J, n)\n",
    "print(\"Custom Weighted MSE Loss:\", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume `LeNet5` and `GaussianGenerator` are already defined and imported\n",
    "\n",
    "# Initialize the models\n",
    "generator = GaussianGenerator(img_dim=(48, 48))\n",
    "net = LeNet5(generator, num_classes=5)\n",
    "\n",
    "# Define the custom loss function\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "    W = tf.pow(I, n)\n",
    "    squared_diffs = tf.pow(I - J, 2)\n",
    "    weighted_squared_diffs = W * squared_diffs\n",
    "    loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "    return loss\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "initial_lr = 1e-6\n",
    "optimizer = Adam(learning_rate=initial_lr)\n",
    "\n",
    "# OneCycleLR is not directly available in Keras, using a custom scheduler instead\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < num_epochs / 2:\n",
    "        return lr * (1 + epoch / (num_epochs / 2)) * (1e-3 / initial_lr)\n",
    "    else:\n",
    "        return lr * (1 - (epoch - num_epochs / 2) / (num_epochs / 2)) * (initial_lr / 1e-3)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200\n",
    "n = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        n += 0.1\n",
    "\n",
    "    for images, image_max in tqdm(data_generator):\n",
    "        images = tf.expand_dims(images, axis=-1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            scaled, unscaled, outputs = net(images, training=True)\n",
    "            loss = custom_weighted_mse_loss(images, outputs, n)\n",
    "        \n",
    "        gradients = tape.gradient(loss, net.trainable_variables)\n",
    "        tf.clip_by_norm(gradients, 1.0)\n",
    "        optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
    "        \n",
    "        running_loss += loss.numpy()\n",
    "        \n",
    "    average_loss = running_loss / len(data_generator)\n",
    "    lr_scheduler.on_epoch_end(epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "    # Save the model if the loss improves\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        net.save('best_model.h5')\n",
    "\n",
    "# Load the best model for inference\n",
    "best_model = tf.keras.models.load_model('best_model.h5', custom_objects={'custom_weighted_mse_loss': custom_weighted_mse_loss})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLS_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
