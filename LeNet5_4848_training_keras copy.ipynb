{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35161 instead\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 234.98 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 229.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 202.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 232.81 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 224.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 224.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 221.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 223.65 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 223.92 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 219.79 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 228.04 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_47547/368317689.py:34: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2)+1e-9)\n",
      "/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 223.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-05-21 18:00:35,509 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56696 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,511 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56698 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,513 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56714 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,513 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56728 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,513 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56676 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,514 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56672 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,515 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56686 remote=tcp://127.0.0.1:39255>: Stream is closed\n",
      "2024-05-21 18:00:35,518 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ferroelectric/micromamba/envs/HLS_new/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56690 remote=tcp://127.0.0.1:39255>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The below code uses the \"traditional\" method to provide close fits that the model uses for standardization later\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class HDF5Dataset(Sequence):\n",
    "    def __init__(self, file_path, batch_size=1000):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as h5:\n",
    "            data = []\n",
    "            growth_numbers = []\n",
    "\n",
    "            # Iterate through the groups and match the pattern\n",
    "            for key in h5.keys():\n",
    "                match = re.match(r'growth_(\\d+)', key)\n",
    "                if match:\n",
    "                    growth_number = int(match.group(1))\n",
    "                    growth_numbers.append(growth_number)\n",
    "\n",
    "                    # Extract the corresponding dataset\n",
    "                    dataset = np.array(h5[key][\"spot_2\"])\n",
    "                    data.append(dataset)\n",
    "                    \n",
    "            # Concatenate all the datasets and normalize\n",
    "            self.data = np.vstack(data)\n",
    "            self.data = self.data.astype(np.float32)\n",
    "            self.data_max = np.max(self.data)\n",
    "            self.data /= self.data_max\n",
    "       \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_data = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return batch_data, np.full(len(batch_data), self.data_max)\n",
    "\n",
    "# Specify the path to the HDF5 file\n",
    "file_path = '/home/ferroelectric/sean/RHEED_4848_test6.h5'\n",
    "\n",
    "# Create an instance of the HDF5Dataset generator\n",
    "batch_size = 1000\n",
    "data_generator = HDF5Dataset(file_path, batch_size=batch_size)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "import h5py\n",
    "\n",
    "# Defining the 2D Gaussian function\n",
    "def gaussian2D(x_y, A, x0, y0, sigma_x, sigma_y):\n",
    "    x, y = x_y\n",
    "    return A * np.exp(-((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2)))\n",
    "\n",
    "\n",
    "# Computing residuals for least-squares minimization\n",
    "def residuals(params, x, y, data):\n",
    "    A, x0, y0, sigma_x, sigma_y = params\n",
    "    model = gaussian2D((x, y), A, x0, y0, sigma_x, sigma_y)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "def convert_parameters(parameters):\n",
    "    A, x0, y0, sigma_x, sigma_y = parameters\n",
    "    \n",
    "    mean_x = x0\n",
    "    mean_y = y0\n",
    "    cov_x = sigma_x\n",
    "    cov_y = sigma_y\n",
    "    \n",
    "    # Calculate theta from the covariance values\n",
    "    if cov_x != 0 and cov_y != 0:\n",
    "        theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2)+1e-9)\n",
    "    else:\n",
    "        theta = 0.0\n",
    "    \n",
    "    return mean_x, mean_y, cov_x, cov_y, theta\n",
    "\n",
    "# Fitting multiple images using Dask\n",
    "def fit_and_convert_parameters(h5_filename):\n",
    "    all_converted_params = []  \n",
    "    all_images=[]\n",
    "    \n",
    "    with h5py.File(h5_filename, 'r') as h5_file:\n",
    "        for group_num in range(1, 13):  \n",
    "            group_name = f'growth_{group_num}'\n",
    "            if group_name in h5_file:\n",
    "                group = h5_file[group_name]['spot_2']\n",
    "\n",
    "                images = group[:]  \n",
    "                \n",
    "                normalized_images = [image / image.max() for image in images]  \n",
    "                all_images.extend(normalized_images)\n",
    "                \n",
    "                guesses = [add_guess(image) for image in normalized_images]\n",
    "                fits = [fit_gaussian2D_delayed(image, guess) for image, guess in zip(normalized_images, guesses)]\n",
    "                \n",
    "                converted_params = [convert_parameters(params) for params in compute(*fits)]\n",
    "                all_converted_params.extend(converted_params)  \n",
    "\n",
    "    return all_converted_params, all_images\n",
    "\n",
    "\n",
    "# Parallelizing the fit function using Dask's delayed\n",
    "@delayed\n",
    "def fit_gaussian2D_delayed(data, guess):\n",
    "    y, x = np.indices(data.shape)\n",
    "    result = least_squares(residuals, guess, args=(x, y, data))\n",
    "    return result.x\n",
    "\n",
    "# Adding an initial guess for the fit\n",
    "def add_guess(data):\n",
    "    A_guess = np.max(data)\n",
    "    x0_guess, y0_guess = np.unravel_index(np.argmax(data), data.shape)\n",
    "    sigma_x_guess = sigma_y_guess = np.std(data)\n",
    "    return [A_guess, x0_guess, y0_guess, sigma_x_guess, sigma_y_guess]\n",
    "\n",
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    h5_filename = \"/home/ferroelectric/sean/RHEED_4848_test6.h5\"  # Replace with the actual path\n",
    "    \n",
    "    with Client() as client:  # Starts a local cluster or connects to an existing one\n",
    "        results ,images2= fit_and_convert_parameters(h5_filename)\n",
    "\n",
    "\n",
    "\"\"\"This is the standard scaler being fit on the traditional data.\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "output_scaler = StandardScaler()\n",
    "data=results #structed like [[meanx1,meany1,covx1,covy1,theta1],[meanx2,meany2,covx2,covy2,theta2]]\n",
    "output_scaler.fit(data)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import math\n",
    "\n",
    "class LeNet5(tf.keras.Model):\n",
    "    def __init__(self, func, num_classes, testing=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.func = func\n",
    "        self.testing = testing\n",
    "        \n",
    "        self.layer1 = tf.keras.Sequential([\n",
    "            Conv2D(6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(pool_size=4, strides=4)\n",
    "        ])\n",
    "        \n",
    "        self.layer2 = tf.keras.Sequential([\n",
    "            Conv2D(16, kernel_size=5, strides=1, padding='valid'),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            MaxPooling2D(pool_size=2, strides=2)\n",
    "        ])\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc = Dense(98)\n",
    "        self.relu = ReLU()\n",
    "        self.fc1 = Dense(52)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        embedding = self.fc2(out)\n",
    "        \n",
    "        # Assume output_scaler is defined and accessible\n",
    "        output_scaler = ...  # Define or load your scaler here\n",
    "        \n",
    "        unscaled_param = embedding * tf.sqrt(output_scaler.var_) + output_scaler.mean_\n",
    "        unscaled_param = tf.Variable(unscaled_param)\n",
    "        unscaled_param[:, 4].assign((math.pi / 4) * (tf.nn.tanh(unscaled_param[:, 4]) + 1))\n",
    "        \n",
    "        final = self.func(unscaled_param)\n",
    "        \n",
    "        if self.testing:\n",
    "            return embedding, unscaled_param, final \n",
    "        \n",
    "        return final\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class LeNet5_inference(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5_inference, self).__init__()\n",
    "        self.layer1 = models.Sequential([\n",
    "            layers.Conv2D(6, kernel_size=5, strides=1, padding='valid', input_shape=(32, 32, 1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling2D(pool_size=4, strides=4)\n",
    "        ])\n",
    "        self.layer2 = models.Sequential([\n",
    "            layers.Conv2D(16, kernel_size=5, strides=1, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "        ])\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(98)\n",
    "        self.relu = layers.ReLU()\n",
    "        self.fc1 = layers.Dense(52)\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "        \n",
    "        self.output_scaler_var = tf.Variable([1.0], trainable=False)  # Placeholder for the variance\n",
    "        self.output_scaler_mean = tf.Variable([0.0], trainable=False)  # Placeholder for the mean\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        embedding = self.fc2(out)\n",
    "        embedding = embedding * tf.sqrt(self.output_scaler_var) + self.output_scaler_mean  # Unscaling\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class GaussianGenerator(Layer):\n",
    "    def __init__(self, img_dim):\n",
    "        super(GaussianGenerator, self).__init__()\n",
    "        self.img_dim = img_dim\n",
    "\n",
    "    def call(self, params):\n",
    "        batch_size = tf.shape(params)[0]\n",
    "        mean_x, mean_y, cov_x, cov_y, theta = tf.unstack(params, axis=-1)\n",
    "        cov_x = tf.clip_by_value(cov_x, clip_value_min=1e-9, clip_value_max=tf.float32.max)\n",
    "        cov_y = tf.clip_by_value(cov_y, clip_value_min=1e-9, clip_value_max=tf.float32.max)\n",
    "\n",
    "        x = tf.range(self.img_dim[1], dtype=tf.float32)\n",
    "        y = tf.range(self.img_dim[0], dtype=tf.float32)\n",
    "        x = tf.reshape(x, (-1, 1))\n",
    "        x = tf.tile(x, (1, self.img_dim[0]))\n",
    "        y = tf.reshape(y, (1, -1))\n",
    "        y = tf.tile(y, (self.img_dim[1], 1))\n",
    "        x = tf.tile(tf.expand_dims(x, 0), [batch_size, 1, 1])\n",
    "        y = tf.tile(tf.expand_dims(y, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        rota_matrix = tf.stack([tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)], axis=-1)\n",
    "        rota_matrix = tf.reshape(rota_matrix, (batch_size, 2, 2))\n",
    "\n",
    "        xy = tf.stack([x - tf.reshape(mean_x, (-1, 1, 1)), y - tf.reshape(mean_y, (-1, 1, 1))], axis=-1)\n",
    "        xy = tf.einsum('bijk,bkl->bijl', xy, rota_matrix)\n",
    "\n",
    "        img = tf.exp(-0.5 * (tf.square(xy[:, :, :, 0]) / tf.square(tf.reshape(cov_x, (-1, 1, 1))) + tf.square(xy[:, :, :, 1]) / tf.square(tf.reshape(cov_y, (-1, 1, 1)))))\n",
    "\n",
    "        return tf.expand_dims(img, axis=1)\n",
    "\n",
    "# Example usage\n",
    "img_dim = (48, 48)  # Replace with the desired image dimensions\n",
    "model = GaussianGenerator(img_dim)\n",
    "sample_params = tf.constant([[19.2763, 24.8520, 11.2061, 6.8914, 0.7006]], dtype=tf.float32)\n",
    "\n",
    "# Call the model to generate the Gaussian image\n",
    "generated_img = model(sample_params)\n",
    "\n",
    "# Display the generated image shape\n",
    "print(\"Generated Image Shape:\", generated_img.shape)\n",
    "plt.imshow(generated_img.numpy().squeeze(0).squeeze(0), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "    \"\"\"\n",
    "    Compute the custom weighted MSE loss between two grayscale images I and J.\n",
    "\n",
    "    Parameters:\n",
    "    - I: tf.Tensor of shape [batch_size, 1, M, N], the input image\n",
    "    - J: tf.Tensor of shape [batch_size, 1, M, N], the target image\n",
    "    - n: int, the exponent to raise the input image for the weight\n",
    "\n",
    "    Returns:\n",
    "    - loss: tf.Tensor, the computed loss\n",
    "    \"\"\"\n",
    "    # Compute the weight\n",
    "    W = tf.pow(I, n)\n",
    "\n",
    "    # Compute the squared differences\n",
    "    squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "    # Compute the weighted squared differences\n",
    "    weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Test the function\n",
    "I = tf.random.normal((16, 1, 128, 128))  # Batch of 16 128x128 grayscale images\n",
    "J = tf.random.normal((16, 1, 128, 128))  # Batch of 16 128x128 grayscale images\n",
    "n = 2  # Exponent value\n",
    "\n",
    "loss = custom_weighted_mse_loss(I, J, n)\n",
    "print(\"Custom Weighted MSE Loss:\", loss.numpy())\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume `LeNet5Keras` and `GaussianGenerator` are already defined and imported\n",
    "\n",
    "# Initialize the models\n",
    "generator = GaussianGenerator(img_dim=(48, 48))\n",
    "net = LeNet5(generator, num_classes=5)\n",
    "# net = net.get_model()\n",
    "\n",
    "# Define the custom loss function\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "    W = tf.pow(I, n)\n",
    "    squared_diffs = tf.pow(I - J, 2)\n",
    "    weighted_squared_diffs = W * squared_diffs\n",
    "    loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "    return loss\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "initial_lr = 1e-6\n",
    "optimizer = Adam(learning_rate=initial_lr)\n",
    "\n",
    "# OneCycleLR is not directly available in Keras, using a custom scheduler instead\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < num_epochs / 2:\n",
    "        return lr * (1 + epoch / (num_epochs / 2)) * (1e-3 / initial_lr)\n",
    "    else:\n",
    "        return lr * (1 - (epoch - num_epochs / 2) / (num_epochs / 2)) * (initial_lr / 1e-3)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200\n",
    "n = 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        n += 0.1\n",
    "\n",
    "    for images, image_max in tqdm(data_loader):\n",
    "        \n",
    "        images = tf.expand_dims(images, axis=-1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            scaled, unscaled, outputs = net(images, training=True)\n",
    "            loss = custom_weighted_mse_loss(images, outputs, n)\n",
    "        \n",
    "        gradients = tape.gradient(loss, net.trainable_variables)\n",
    "        tf.clip_by_norm(gradients, 1.0)\n",
    "        optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
    "        \n",
    "        running_loss += loss.numpy()\n",
    "        \n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    lr_scheduler.on_epoch_end(epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "    # Save the model if the loss improves\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        net.save('best_model.h5')\n",
    "\n",
    "# Load the best model for inference\n",
    "best_model = tf.keras.models.load_model('best_model.h5', custom_objects={'custom_weighted_mse_loss': custom_weighted_mse_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 18:02:35.257476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 18:02:35.821650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image Shape: (1, 1, 48, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 18:02:38.870193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.870493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.887907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.888137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.888317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.888470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:38.993829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.001964: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-05-21 18:02:39.002022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22437 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-05-21 18:02:39.002296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-21 18:02:39.002422: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 1\n",
      "2024-05-21 18:02:39.002458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22453 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:4c:00.0, compute capability: 8.6\n",
      "2024-05-21 18:02:39.066690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Weighted MSE Loss: 3.994163\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, image_max \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     54\u001b[0m     images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "File \u001b[0;32m~/micromamba/envs/HLS_new/lib/python3.10/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
